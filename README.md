
在机器学习领域，根据反馈的不同，可以分为监督学习、非监督学习和强化学习三大类。强化学习是指一个能感知环境的自治Agent，通过试错（trial-and-error）的方法，学习出从环境状态到行为的最优映射，以使Agent从环境中获得的收益最大化

## 基本模型
![图1][1]
强化学习的基本模型可用上图来描述：Agent依据环境状态State，从自己的行为集合中选择操作action反馈给环境；环境依据当前状态state和操作action，通过一个状态转移函数T(s,a,s')跃迁到一个新的状态s'，至此完成了一步迭代。每次迭代完成，Agent将收到环境反馈的收益并以此作为依据不断调整自身的策略。这样通过多次的迭代，Agent会慢慢学习到一个策略，对于每种状态，都能给出收益最优的操作。


## 基本假设
实际应用中Agent所处的环境可能是非常复杂的，为了简化，在基本模型中作了以下假设
1、环境是非确定性的，即状态转移函数T输出的是一个概率值，S*A->S是一种概率映射
2、环境是平衡的，即状态转移函数T输出的分布是稳定的，或至少短时平衡或缓慢变化的

## 优化目标
增强学习最终输出的是一个最优的行为策略，所以归根到底还是个优化问题，优化的目标函数通常有如下三种：
### 1. Finite horizon Model
$$E(\sum_{t=0}^h r_t)$$
有限视野的优化，它只考虑最近h步的收益期望，这个比较符合人做判断的方式，比如下象棋时的走一想三就是考虑了后续几步收益所给出的最优走法。
### 2. Infinite horizon Model
$$E(\sum_{t=0}^\infty\gamma^t r_t)$$
带衰减的无穷视野收益期望，无限的情况存在于理论的世界里，离当前时间点越远其重要性越低的假设既符合主观认识又能保证数学上的收敛，这也是最常用的形式。
### 3. Average reward Model
$$\lim_{h\rightarrow\infty}E(\frac{1}{h}\sum_{t=0}^h r_t)$$
无穷视野的另一个版本，考虑每步得到的期望平均收益。

## 马尔科夫决策过程（MDP）
许多强化学习方法都基于一种假设，即Agent与环境的交互可用一个马尔可夫决策过程（MDP）来刻画。
1. Agent和环境可以刻画为同步的有限状态自动机
2. Agent和环境在离散的时间段内交互
3. Agent能感知环境的状态并做出反应动作
4. Agent动作执行完后，环境的状态会作出改变
5. Agent执行完后，会得到某种回报

MDP符号化地定义一个多状态带时延反馈的增强学习过程，它由一个五元组构成$(S,A,{T}_{sa},\gamma,R)$

 - S：状态集合（States）
 - A：动作集合（Actions）
 - ${T}_{sa}：S \times A \rightarrow S   $：状态转移函数，即在s状态下，采取动态a，会转移到其它状态的概率。
 - $\gamma\in(0,1)$ 是阻尼系数（Discount Factor）
 - $R:S \rightarrow R$ 是回报函数

MDP的动态过程如下：某个Agent所处环境初始状态为s0，然后从A中选择一个动作a0，执行后按概率随机转移到下一个状态s1，然后再执行一个动作a1，转移到状态s2...。其M本质在于：环境在任意时刻t的状态为s，Agent执行动作a发生后环境转变到某个下一状态s'的可能性仅仅依赖于状态s和动作a，而不依赖于时间和过去的状态。"将来"与"现在"有关，而与"过去"无关。

##模型求解
MDP模型的求解很简单，要输出的最优策略其实是一个状态到操作的映射函数S->A，即在什么状态下应采用什么操作是确定的（根据短时平稳性假设，至少是短时确定的）。为得到这样的策略，我们首先需要定义在状态s时不同操作所能获得的期望收益，并把这个收益最大化。
$$V^*(s)=\max_{a}(R(s,a)+\sum_{s'\in S}T(s,a,s')V^*(s'))$$
获得最大收益时对应的策略
$$\pi^*(s)=arg\max_{a}(R(s,a)+\sum_{s'\in S}T(s,a,s')V^*(s'))$$
上面两个式子，分别称为”值函数“和”策略函数“，在R和T已知的情况下，可以通过初始化，然后迭代求解使结果收敛。对值函数的迭代的称为“值迭代”，对策略函数进行迭代称为“策略迭代”，两种算法结果一致，性能稍有差别。值迭代后续又演化出了增强学习最具代表性的算法“Q-Learning”。
###算法1：值迭代
 1. 随机初始化值函数 V(S)
 2. 更新 $$V(s)=\max_{a}(R(s,a)+\sum_{s'\in S}T(s,a,s')V^*(s'))$$
 3. 重复2直到收敛

###算法2：策略迭代
 1. 随机初始化策略$\pi$
 2. 求解线性方程组：$$V_{\pi}(s)=R(s,\pi(s))+\gamma\sum_{s'\in S}T(s,\pi(s),s')V_{\pi}(s')$$
 3. 更新策略 $$\pi(s):=arg\max_{a}(R(s,a)+\gamma\sum_{s'\in S}T(s,a,s')V_{pi}(s))$$
 4. 重复2、3直至收敛
 
MDP是一个很理想化的模型，它假设了我们事先对环境的完全了解（转移概率函数T及反馈函数R），而增强学习要解决的问题恰恰是如何去应对未知的环境。虽然MDP模型并不具有现实可用的意义，但通过它我们定义了一整套的符号系统以及问题解决的途径，极大地简化了对问题的描述与求解。在理论上解决了一些抽象层面的问题后（如收敛性、计算复杂度），要把它推广到现实的应用，只要着眼于解决对T及R函数的估计或替代即可。那些看似没有实用价值的数学模型的意义就在于此。

##一般情况下MDP的求解
在现实应用中，MDP的环境模型（T及R函数）的参数是未知的，所以增强学习的过程可以化归为一个参数估计问题，只要能通过逐步得到的数据迭代地修正模型参数或算法参数，就可以直接利用通过MDP推导得到的结果。要实现这个目的，也有两种方式:

 - model-based（学习模型的参数并把估计得到的模型应用于策略输出），常见算法Dyna
 - model-free（不断根据数据输入调整策略的输出，而无需估计具体的模型），常见算法$TD(\lambda)$和Q-learning
 
两种实现方式各有千秋，一般来说model-free的方法需要更多的迭代步数才能收敛，但需要更少的环境知识，而model-based的方法则正好相反。

##增强学习潜在的应用场景

由于增强学习解决的是一类问题，所以它在自适应控制、机器人控制、博弈等等领域都有广泛的应用。从增强学习与有监督学习的对比可以看到，增强学习特别适用于那些没有启动数据的环境。
